{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    est = str(estimator)\n",
    "    if est.find('ComplementNB')>=0 or est.find('ComplementNB')>=0:\n",
    "        X_train_fs = fs_selector.transform(X_train)\n",
    "        # when Classifiers don't have 'get_support()' - then call this\n",
    "        top_feat_fs = get_selected_columns(X_train, X_train_fs, labels, show_compare=True)\n",
    "    else:\n",
    "        # non GaussianNB\n",
    "        X_train_fs = fs_selector.transform(X_train)\n",
    "        top_feat_fs = get_selected_columns(X_train, X_train_fs, labels, show_compare=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # combine final \"test\" Id column with prediction probabilities column (cover to dataframe first) \n",
    "    frames = [X_final.iloc[:,0], pd.DataFrame(y_final_preds[:,1])]\n",
    "    result = pd.concat(frames, axis=1) \n",
    "    result.columns = ['Id','tmp']\n",
    "    result['TARGET_5Yrs'] = [round(num, 2) for num in result['tmp']]\n",
    "    result.drop(['tmp'], axis=1, inplace=True)\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # Save the final predictions for submission to Kaggle\n",
    "    result.to_csv('../data/processed/group1_r{d}_{c}.csv'.format(d=test_no, c=classifier_name), index=False)\n",
    "    #--------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_kaggle_output(classifier, X_train, Y_train, X_final)\n",
    "\n",
    "    t_start = time.process_time()\n",
    "    # fit model\n",
    "    classifier.fit(X_train, Y_train)\n",
    "    t_end = time.process_time()       \n",
    "    t_diff = t_end - t_start\n",
    "    \n",
    "    \n",
    "    #save the final \"test\" prediction probabilities for Kaggle\n",
    "    y_final_preds = classifier.predict_proba(X_final)\n",
    "\n",
    "    # combine final \"test\" Id column with prediction probabilities column (cover to dataframe first) \n",
    "    frames = [X_final.iloc[:,0], pd.DataFrame(y_final_preds[:,1])]\n",
    "    result = pd.concat(frames, axis=1) \n",
    "    \n",
    "    print('result',result)\n",
    "    \n",
    "    result.columns = ['Id','tmp']\n",
    "    result['TARGET_5Yrs'] = [round(num, 2) for num in result['tmp']]\n",
    "    result.drop(['tmp'], axis=1, inplace=True)\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # Save the final predictions for submission to Kaggle\n",
    "    result.to_csv('../data/processed/group1_r{d}_{c}.csv'.format(d=test_no, c=classifier_name), index=False)\n",
    "    #--------------------------------------------------------------------------\n",
    "    # prepare data same as for modelling - labels = all features\n",
    "    #df_clean_mod, df_clean_mod_test = select_features(df_fix_train, df_fix_test, labels)\n",
    "    #X_train, y_train, X_val, y_val, X_test, y_test = Prep_Model_Data(df_clean_mod, target, scaler=scaler, resample='', random_state=random_state)\n",
    "\n",
    "    # Run SelectModel Feature Selection for the each Classifier - as they return different number of features\n",
    "    #top_feat = select_features_auto(X_train, y_train, feature_selector='SFM', no_select_feat=2, estimator=classifier, max_features=i, step=1, cv=5, direction='forward')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/5-feature-selection-method-from-scikit-learn-you-should-know-ed4d116e4172\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "t_start = time.process_time()\n",
    "\n",
    "# Selecting the Best important features according to Logistic Regression\n",
    "sfs_selector = SequentialFeatureSelector(estimator=LogisticRegression(), n_features_to_select=3, cv=5, direction ='backward')\n",
    "sfs_selector.fit(df_fix_train, target)\n",
    "\n",
    "print(sfs_selector)\n",
    "\n",
    "top_feat_sfs = df_fix_train.columns[sfs_selector.get_support()]\n",
    "print(top_feat_sfs)\n",
    "print('Time taken:', (time.process_time()-t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature Selection via SelectFromModel\n",
    "# https://towardsdatascience.com/5-feature-selection-method-from-scikit-learn-you-should-know-ed4d116e4172\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "t_start = time.process_time()\n",
    "\n",
    "# Selecting the Best important features according to Logistic Regression using SelectFromModel\n",
    "sfm_selector = SelectFromModel(estimator=LogisticRegression(), max_features=3)\n",
    "sfm_selector.fit(df_fix_train, target)\n",
    "\n",
    "print(sfm_selector)\n",
    "\n",
    "top_feat_sfm = df_fix_train.columns[sfm_selector.get_support()]\n",
    "print(top_feat_sfm)\n",
    "print('Time taken:', (time.process_time()-t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Trials, STATUS_OK, tpe, hp, fmin from hyperopt package\n",
    "from hyperopt import Trials, STATUS_OK, tpe, hp, fmin\n",
    "\n",
    "# Define the search space for xgboost hyperparameters\n",
    "space = {\n",
    "    'max_depth' : hp.choice('max_depth', range(5, 20, 1)),\n",
    "    'learning_rate' : hp.quniform('learning_rate', 0.01, 0.5, 0.05),\n",
    "    'min_child_weight' : hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'subsample' : hp.quniform('subsample', 0.1, 1, 0.05),\n",
    "    'colsample_bytree' : hp.quniform('colsample_bytree', 0.1, 1.0, 0.05)\n",
    "}\n",
    "\n",
    "# Define a function called objective\n",
    "def objective(space):\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "    xgboost = xgb.XGBClassifier(\n",
    "        max_depth = int(space['max_depth']),\n",
    "        learning_rate = space['learning_rate'],\n",
    "        min_child_weight = space['min_child_weight'],\n",
    "        subsample = space['subsample'],\n",
    "        colsample_bytree = space['colsample_bytree'],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    # X_train_res with selected features only\n",
    "    acc = cross_val_score(xgboost, X_train_res, y_train_res, cv=10, scoring=\"accuracy\").mean()\n",
    "\n",
    "    return{'loss': 1-acc, 'status': STATUS_OK }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch Hyperopt search and save the result in a variable called `best`\n",
    "best = fmin(\n",
    "    fn=objective,   \n",
    "    space=space, \n",
    "    algo=tpe.suggest,       \n",
    "    max_evals=10\n",
    ")\n",
    "\n",
    "# Print the best set of hyperparameters\n",
    "print(\"\\nBest: \", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_LogisticRegression(clf_lr, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    # Fit the model with the prepared data\n",
    "    clf_lr.fit(X_train, y_train)\n",
    "    print(clf_lr.fit(X_train, y_train),'\\n')\n",
    "\n",
    "    # Import dump from joblib and save the fitted model into the folder models as a file called xgboost_default\n",
    "    from joblib import dump \n",
    "    dump(clf_lr,  '../models/clf_lr_default.joblib')\n",
    "\n",
    "    # Save the predictions from this model for the training and validation sets into 2 variables called y_train_preds and y_val_preds\n",
    "    y_train_preds = clf_lr.predict(X_train)\n",
    "    y_val_preds = clf_lr.predict(X_val)\n",
    "    y_test_preds = clf_lr.predict(X_test)\n",
    "    \n",
    "    # prediction probabilities for AUC\n",
    "    y_train_pred_prob = clf_lr.predict_proba(X_train)\n",
    "    y_val_pred_prob = clf_lr.predict_proba(X_val)\n",
    "    y_test_pred_prob = clf_lr.predict_proba(X_test)\n",
    "\n",
    "    print_class_perf(y_preds=y_train_preds, y_actuals=y_train, y_pred_prob=y_train_pred_prob[:,1], set_name='Training', average='weighted')\n",
    "    print_class_perf(y_preds=y_val_preds, y_actuals=y_val, y_pred_prob=y_val_pred_prob[:,1], set_name='Validation', average='weighted')    \n",
    "    print_class_perf(y_preds=y_test_preds, y_actuals=y_test, y_pred_prob=y_test_pred_prob[:,1], set_name='Test', average='weighted')\n",
    "    \n",
    "    return clf_lr, clf_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a LogisticRegression with best set of hyperparameters\n",
    "lr_2 = LogisticRegression(\n",
    "    C = best['C'],\n",
    "    warm_start = warm_start[best['warm_start']],\n",
    "    fit_intercept = fit_intercept[best['fit_intercept']],\n",
    "    tol = best['tol'],\n",
    "    solver = solver_lr[best['solver']],\n",
    "    max_iter = best['max_iter'],\n",
    "    intercept_scaling = intercept_scaling[best['intercept_scaling']],\n",
    "    random_state=random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Trials, STATUS_OK, tpe, hp, fmin from hyperopt package\n",
    "from hyperopt import Trials, STATUS_OK, tpe, hp, fmin\n",
    "solver_lr = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']\n",
    "warm_start = [True, False]\n",
    "fit_intercept = [True, False]\n",
    "intercept_scaling = [0, 1]\n",
    "multi_class = ['auto']\n",
    "class_weight = ['balanced']\n",
    "\n",
    "# Define the search space for xgboost hyperparameters\n",
    "space_lr = {\n",
    "    'C' : hp.uniform('C', 0.05, 3),\n",
    "    'warm_start' : hp.choice('warm_start', [True, False]),\n",
    "    'fit_intercept' : hp.choice('fit_intercept', [True, False]),\n",
    "    'tol' : hp.uniform('tol', 0.00001, 0.0001),\n",
    "    'solver' : hp.choice('solver', ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']),\n",
    "    'max_iter' : hp.choice('max_iter', range(100,1000)),\n",
    "    'intercept_scaling': hp.choice('intercept_scaling', [0, 1])\n",
    "}\n",
    "\n",
    "# Define a function called objective\n",
    "def objective_lr(space_lr):\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "    clf_lr = LogisticRegression(\n",
    "        C = space_lr['C'],\n",
    "        warm_start = space_lr['warm_start'],\n",
    "        fit_intercept = space_lr['fit_intercept'],\n",
    "        tol = space_lr['tol'],\n",
    "        solver = space_lr['solver'],\n",
    "        max_iter = space_lr['max_iter'],\n",
    "        intercept_scaling = space_lr['intercept_scaling'],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    # X_train_res with selected features only\n",
    "    acc = cross_val_score(clf_lr, X_train_res, y_train_res, cv=10, scoring=\"accuracy\").mean()\n",
    "\n",
    "    return{'loss': 1-acc, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch Hyperopt search and save the result in a variable called `best`\n",
    "best = fmin(\n",
    "    fn=objective_lr,   \n",
    "    space=space_lr, \n",
    "    algo=tpe.suggest,       \n",
    "    max_evals=10\n",
    ")\n",
    "\n",
    "# Print the best set of hyperparameters\n",
    "print(\"\\nBest: \", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
